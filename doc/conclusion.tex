\section{Summary and Conclusions}
 This chapter summarizes the results achieved by the thesis and presents a discussion of these results. In the remainder of this chapter an outlook to future development is given.
 
 
 \subsection{Summary}
  The aim of this thesis was to develop a FEM-code being able to be coupled in a fluid-structure interaction. The program development should be supported by a FEM framework. Comparison aspects had to be created and an evaluation of several FEM libraries was performed. The implemented FEM code was to be validated with example problems. The coupling part should be managed by the preCICE tool. The overall focus was on creating a well documented and easily readable and maintainable code, applicable for multi-physics simulations and qualified for future development and extensions.

  For the FEM framework evaluation, different comparison aspects were introduced. Besides organizational requirements like an open-source code, C++ as development programming language and a wide and accurate documentation of the framework, numerical and programmatic aspects were considered. The last aspects included the possibility to parallelize the code via MPI, a large collection of finite element types and built-in iterative linear solvers. During the evaluation process two libraries were practically tested, the first one being ``MFEM''. Because of difficulties in use, another library named ``libMesh'' was tested and finally chosen for the program development.  

  In this thesis flat shell elements were implemented. Such an element is composed of a plane and a plate element part that are superimposed in order to construct the final shell element. Two different types of finite elements were considered: A three-node triangular element, denoted as ``Tri-3'' and a four-node quadrilateral element, denoted as ``Quad-4''. Six models had to be implemented: A plane, plate and shell element for each of the two discretizations. Therefore, existing models like the Discrete Kirchhoff Quadrilateral were selected for implementation.

  The libMesh framework supported the development of the FEM-code by providing many components and classes that only needed to be configured and put into the program like building bricks. The import of mesh files and the construction of the internal representation of the mesh as nodes, edges and elements is done by libMesh. Also, the creation of a solver and the boundary conditions is simplified by using predefined classes. The solving of the system is done by libMesh or by the external library PETSc which is required as soon as the program is executed in parallel. The major task was to create the assembly function that constructs the overall system matrix and right-hand side. Here, every element was transformed from global space to a local coordinate system in order to assemble its local stiffness matrix. The local stiffness matrices were then re-transformed into global space in order to add them to the global system matrix. The adding step as well as the constraining of elements at the boundary is performed by libMesh on the other hand. Due to the fact that MPI is integrated in libMesh throughout the library, the parallelization of the program only required minor modifications to the code, although additional build time requirements for libMesh for the external library PETSc were not evident in the beginning.
  The coupling tool preCICE was used to create a second version of the program that is capable of being coupled with other solvers in a multi-physics simulation. preCICE serves as connector between the single solvers, managing data mapping and communication, for instance. One goal of preCICE is to require only minimal changes to the solver's code in order to integrate its API and make it coupling-ready. This was confirmed, as only a few lines of code had to be changed besides additional data structures necessary for inter-solver communication.

  Every implemented element model was validated with respect to accuracy. In addition to that, the convergence was tested, i.e. the behavior of accuracy with an increasing level of mesh refinement. The parallelization was tested with an unchanged example problem, solved by an increasing number of processes. The execution time of different code parts like the matrix assembly or the solving step was then measured. The coupling via preCICE was tested with two example problems. One was a fluid-structure interaction with the developed program and an external fluid solver as participants. In the test, a flow was driven through a channel with an elastic tower-like structure fixed at the bottom side of the channel as deformable obstacle for the flow.
 
 \subsection{Conclusion}
  The evaluation of FEM frameworks yield two suitable libraries for the program's development. The first framework that was used was ``MFEM''. A problem in the practical use made it necessary to switch to another library: Two dimensional geometric elements are processed with the thesis' program. These elements can be positioned arbitrarily in the three dimensional space. The definition of 2D elements in a mesh file that is specified to be in 3D space, led to undefined behavior and program crashes when using MFEM. If 2D elements were specified with 3D coordinates in a mesh file that were specified to be in 2D space, the third coordinate component was simply ignored. The solution to this problem would have lead to another problem with the definition of boundary conditions at the element's nodes. Therefore, the second library was taken into account. The ``libMesh'' framework offered basically the same features as MFEM, although it has other specializations like the focus on adaptive mesh refinements and parallelization. With the help of this framework, the program could be developed without any major issues. The possibilities of libMesh qualify for further use in future development of the structure solver.
  
  In order to offer an appropriate structure solver for coupling purposes, all element components used for the implementation of flat shell elements were validated separately. Hence, several example problems were selected that provided either an analytical solution or solutions created by approved software. The tests of the Tri-3 and Quad-4 plane elements showed very good accuracy compared to the commercial software ``SAP2000'' with a difference of less than $0.03\%$ between the displacement values. The first example problem also illustrated that the arrangement of elements in the mesh has an important part to accuracy. The test of the Tri-3 plate element indicated a first impression of the involvement of mesh subdivisions: While the accuracy difference was only around $9\%$ for a $4\!\times\!4$ mesh refinement, the accuracy increased to only around $1\%$ difference to the exact value. Another factor involved in the accuracy got visible in the validation of the Quad-4 plate element: The type of loading has different influence to the accuracy of the example problem. A uniform load, where the forces are distributed throughout the whole plate, leads to good accuracy even with coarse mesh refinement. For a concentrated loading applied on only one node at the center of the plate, the mesh refinement must be increased to gain the same level of accuracy. This behavior can only be observed with plate elements and is due to the chosen finite element models. A eight-node quadrilateral element would approximate edge twists better and therefore lead to more accurate results. The combinations of plane and plate elements produce the flat shell elements. These were tested with an example problem where an I-beam was fixed at the one end and nodal forces were applied at the other end, facing inwards to the middle of the beam. This results in deformations in all three spatial axes. The accuracy of the Tri-3 element was better compared to the Quad-4 element, although the differences among the $x$-, $y$- and $z$-displacement is larger compared to the quadrilateral element.

  In a separate series of tests the element's convergence with respect to accuracy was validated. The level of mesh subdivisions was increased from test to test. While all tests showed that the higher the mesh refinement is the better the analytical solution is approximated, the test with uniform loading and clamped boundary conditions sticks out: The accuracy stagnates around $1.7\%$ difference to the exact value independently from the number of elements forming the plate. This behavior cannot be explained and might only be specific to the tested scenario. The other tests confirmed previous observations: With clamped boundary conditions, the solutions show small differences to the analytical result if the mesh is subdivided very fine. With a uniform loading, the accuracy is high even for coarse meshes, while concentrated loadings need finer meshes to gain the same level of accuracy. 
  
  The parallelization of the program was also tested. For this, three different times were measured while executing the program: The time needed to assembly the system matrix and right-hand side, the time to solve the system and the overall time from the beginning of the program to the last line of code. The test must be split in halves, since it contains code that is outside of the scope of this thesis: The system solving is done by PETSc in parallel. The benefits with respect to time saving can only be observed, not motivated. The other half is the assembly of the system, which is controlled by the thesis' program. Here, every element of the mesh must be processed. This task can be efficiently partitioned between the single processes. Therefore, the time for assembling the matrix is nearly halved from the sequential execution to the execution with two processes. This would lead to a speedup of around $100\%$ for this part of the code and without further tests with more processes. Since every element can be processed predominantly independent from the others, this speedup value is possible, though. The bottleneck in this case is the adding of the local matrices to the overall system matrix that is done by libMesh, because the local matrix entries can be have distributed positions in the overall matrix. The solver behaves less efficient compared to the assembly function. Here, the benefit of multiple processes is also visible, but not as significant as before. The tests were performed on an Intel dual-core processor that uses hyper-threading. This technology lead to even shorter times when both, physical and logical cores are active. Though, the two additional logical cores cannot replace a physical quad-core processor. This is especially noticeable when using three processes, i.e. two physical cores and one logical core: The computation times even rise compared to two processes but still stay below the single-threaded run. When using three processes, it is imaginable that either the operating system or the processor controller have difficulties to handle the extra logical core besides the two physical cores that are used. This might lead to extra managements like communication, memory transfer or context switches that slows down the execution time. In summary, one can state that the solver is capable of being executed in parallel and offers good performance with multiple processes. The solver takes the biggest part of the runtime, at least for meshes with a large number of nodes. For small meshes, the time to import the mesh, initialize the system and write the outputs - which would otherwise need short time - dominates the execution time, instead of the system's assembly or its solving.
  
  The last two tests were coupled simulations, the first one between the thesis' program and a fluid solver dummy, the second one a fluid-structure interaction, where the developed program was coupled with an external fluid solver written with the OpenFOAM fork ``foam-extend''. In both tests, the participants were connected via the preCICE tool. The intention behind the test with the fluid solver dummy was to generally test the coupling with preCICE. The forces produced by the dummy solver resulted from algebraic and trigonometric functions and were dependent from the current time step only, not from the displacements of the structure solver. Therefore, this test cannot be seen as a fluid-structure interaction. But the example showed successfully that the developed program can be coupled via preCICE with a totally independent and different solver and is stable throughout the simulation. The parallel execution of both participants steered by preCICE was also successful. For the second coupling test, an external fluid solver developed with ``foam-extend'' was used in a fluid-structure interaction. Although the simulation configuration is still to be adjusted at the time this thesis is completed, the results already showed a successful coupling between the two solver through preCICE.

  As a resum\'{e} one can state that the developed structure solver is qualified to participate in multi-physics simulations. The accuracy of the implemented models is good, even for a coarse mesh, but can be increased by additional refinement of the mesh. The good performance in parallel scaling makes it possible to provide such a fine subdivision level without spending too much time for the processing of the increased number of elements. The integration of preCICE in the program code was successful, as seen by the two coupling tests. The thesis' program was able to be coupled with independent solvers in a single-threaded setup as well as in parallel execution.

 \subsection{Future Development}
  The thesis' program was designed to be easily extensible with new models for plane and plate elements, for example quadratic or cubic quadrilateral elements with 8 or 9 nodes. Such elements would enhance the accuracy, especially for coarser meshes. At the moment, the program can only process two dimensional elements (Tri-3 and Quad-4). One could expand this also to bar and beam elements of one dimension or to three dimensional solid elements represented by tetrahedra or hexahedra, for instance. The libMesh library provides such elements and the coupling tool preCICE support data exchange of one or three dimensional vector data.
  
  All influences to the structures considered in this work were seen as static. But there are scenarios where the temporal change influences the behavior of the displacements and stresses. The dynamic reaction of buildings to wind or earthquakes are only two examples. Here, the inertial force of the structure results from the acceleration of masses. If the external influences to the structure changes quickly in time, the inertial forces become considerable. Simultaneously to the inertia, damping forces can be observed that lead to a decay of the free oscillation. In order to model this dynamic, time-dependent behavior, the current system must be extended by a mass matrix and damping matrix, as well as node velocities and accelerations, which are then dependent on the time.