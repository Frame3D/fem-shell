\section{Summary and Conclusions}
 This chapter summarizes the results achieved by the thesis and presents a discussion of these results. In the remainder of this chapter an outlook to future development is given.
 
 
 \subsection{Summary}
  The aim of this thesis was to develop a FEM-code being able to be coupled in a fluid-structure interaction. The program development should be supported by a FEM framework. Comparison aspects had to be created and an evaluation of several FEM libraries was performed. The implemented FEM-code was to be validated with example problems. The coupling part should be managed by the preCICE tool. The overall focus was on creating a well documented and easily readable and maintainable code, applicable for multi-physics simulations and qualified for future development and extensions.

  For the FEM framework evaluation, different comparison aspects were introduced. Besides organizational requirements like an open-source code, C++ as development programming language and a wide and accurate documentation of the framework, numerical and programmatic aspects were considered. The last aspects included the possibility to parallelize the code via MPI, a large collection of finite element types and built-in iterative linear solvers. During the evaluation process two libraries were practically tested, the first one being ``MFEM''. Because of difficulties in use, another library named ``libMesh'' was tested and finally chosen for the program development.  

  In this thesis flat shell elements were implemented. Such an element is composed of a plane and a plate element part that are superimposed in order to construct the final shell element. Two different types of finite elements were considered: A three-node triangular element, denoted as ``Tri-3'' and a four-node quadrilateral element, denoted as ``Quad-4''. Six models had to be implemented: A plane, plate and shell element for each of the two discretizations. Therefore, existing models like the Discrete Kirchhoff Quadrilateral were selected for implementation.

  The libMesh framework supported the development of the FEM-code by providing many components and classes that only needed to be configured and put into the program like building bricks. The import of mesh files and the construction of the internal representation of the mesh as nodes, edges and elements is done by libMesh. Also, the creation of a solver and the boundary conditions is simplified by using predefined classes. The solving of the system is done by libMesh or by the external library PETSc which is required as soon as the program is executed in parallel. The major task was to create the assembly function that constructs the overall system matrix and right-hand side. Here, every element was transformed from global space to a local coordinate system in order to assemble its local stiffness matrix. The local stiffness matrices were then re-transformed into global space in order to add them to the global system matrix. The adding step as well as the constraining of elements at the boundary is performed by libMesh on the other hand. Due to the fact that MPI is integrated in libMesh throughout the library, the parallelization of the program only required minor modifications to the code, although additional build time requirements for libMesh for the external library PETSc were not ERSICHTLICH ...able in the beginning. %TODO HIER FEHLT EIN WORT!
  The coupling tool preCICE was used to create a second version of the program that is capable of being coupled with other solvers in a multi-physics simulation. preCICE serves as connector between the single solvers, managing data mapping and communication, for instance. One goal of preCICE is to require only minimal changes to the solver's code in order to integrate its API and make it coupling-ready. This was confirmed, as only a few lines of code had to be changed besides additional data structures necessary for inter-solver communication.

  Every implemented element model were validated with respect to accuracy. In addition to that the convergence was tested, i.e. the behavior of accuracy with an increasing level of mesh subdivision. The parallelization was validated with the same example problem solved by an increasing number of processes and measuring the execution time of different code parts like the matrix assembly or the solving step. The coupling was tested with an fluid-structure interaction problem with the developed program and an OpenFOAM fluid solver as participants. In the test a flow was driven through a pipe with an elastic structure fixed at the bottom side of the tube as deformable obstacle for the flow.
 
 \subsection{Conclusion}
  The evaluation of FEM frameworks yield two suitable libraries for the program's development. The first framework that was used was ``MFEM''. A problem in the practical use made it necessary to switch to another library. Two dimensional geometrical elements are processed within the scope of the thesis' program. These elements can be positioned arbitrarily in the three dimensional space. The definition of 2D elements in a mesh file that is specified to be in 3D space lead to undefined behavior or even program crashes when using MFEM. If 2D elements were specified with 3D coordinates in a mesh file that were specified to be in 2D space, the third coordinate component was ignored. Every tested solution to this problem brought new problems and therefore the second suitable library was taken into account. The ``libMesh'' framework offered basically the same features as MFEM although it has other specializations like the focus on adaptive mesh refinements. With the help of this framework the program could be developed without any major issues. The possibilities of libMesh qualify for further use in future development of the structure solver.
  
  In order to offer an appropriate structure solver for coupling purposes, all element components used for the implementation of flat shell elements were validated separately. Hence, several example problems were selected that provided either an analytical solution or solutions created by commercial and approved software. The tests of the Tri-3 and Quad-4 plane elements showed very good accuracy compared to the commercial software SAP2000 with a difference of less than $0.03\%$. The first example problem also illustrated that the arrangement of elements in the mesh has an important part in accuracy. The test of the Tri-3 plate element indicated a first impression of the involvement of mesh subdivisions: While the accuracy was only around $9\%$ for a $4\!\times\!4$ mesh refinement, the accuracy increased to only around $1\%$ difference to the exact value. Another involved factor got visible in the Quad-4 plate element test: The type of loading has different influence to the accuracy of the example problem. A uniform load where the forces are distributed throughout the whole plate leads to good accuracy even with coarse mesh refinement. For a concentrated loading applied on only one node at the center of the plate, the mesh refinement must be increased to gain the same level of accuracy. This behavior can only be observed with plate elements and is due to the chosen finite element models. A eight-node quadrilateral element would approximate edge twists better and therefore lead to more accurate results. The combinations of plane and plate elements produce a shell elements. These were tested with an example problem where an I-beam was fixed at the one end and nodal forces were applied at the other end facing inwards to the middle of the beam. This results in deformations in all three spatial axes. The accuracy of the Tri-3 element was better compared to the Quad-4 element although the difference between the $x$-, $y$- and $z$-displacement is larger when compared to the quadrilateral element.

  In a separate series of tests the element's convergence with respect to accuracy was validated. The level of mesh subdivisions was increased from test to test. While all tests showed that the higher the mesh refinement is the better the analytical solution is approximated by the solver, the test with uniform loading and clamped boundary conditions sticks out: The accuracy stagnates around $1.7\%$ difference to the exact value independently from the number of elements forming the plate. This behavior cannot be explained and might only be specific to this scenario. The other tests confirmed previous observations: With a uniform loading the accuracy is high even for coarse meshes while concentrated loadings need finer meshes. With clamped boundary conditions the solutions show large difference to the analytical result if the mesh is not very fine subdivided.
  
  The parallelization of the program was validated in an additional test. For this, three different times were measured while executing the program: The time needed to assembly the system matrix and right-hand side, the time to solve the system and the overall time from the beginning of the program to the last line of code. This validation must be split in halves since it contains code that is outside of the scope of this thesis: The solving is done by PETSc in parallel. The benefits with respect to time saving can only be observed not explained. The other half is the assembly of the system. Here, every element of the mesh must be processed. This task can be efficiently partitioned and lend to the single processes. Therefore the time for assembling the matrix is nearly halved from the sequential to the execution with two processes. This would lead to a speedup of around $100\%$ for this part of the code and without further tests with more processes. Since every element can predominantly be processed independently from the others this speedup value is possible, though. The bottleneck in this case is the adding of the local matrices to the overall system matrix that is done by libMesh. The solver behaves less efficient compared to the assembly function. Here, the benefit of multiple processes is also visible, but not as dominant as before. The tests were performed on an Intel dual-core processor that uses hyper-threading. This technology lead to even shorter times when both, physical and logical cores are active. Though, the two additional logical cores cannot replace a real quad-core processor. This is especially noticeable when using three processes, i.e. the two physical cores and one logical core: The computation times even rise compared to only two processes but still stay below the single-threaded run. When using three processes it is imaginable that either the operating system or the processor controller have difficulties to handle the extra logical core besides the two physical cores that are used. This could lead to extra managements like communication, memory transfer or context switches that slows down the execution time. All in all one can state that the solver is capable of being executed in parallel and offers good performance with multiple processes. The solver takes the biggest part of the runtime at least for big meshes. For very small meshes the time to import the mesh, initialize the system and write the outputs - which would otherwise cost minimal time - dominates the execution time.
  
  The last two validation tests were coupled simulations, the first one between the thesis' program and a fluid solver dummy, the second one a fluid-structure interaction where the developed program was coupled with a fluid solver written with the OpenFOAM framework. In both tests, the participants were connected via the preCICE tool. The intention behind the test with the fluid solver dummy was to generally validate the coupling with preCICE. The forces produced by the dummy solver resulted from algebraic and trigonometric functions and were only dependent from the current time step, not from the displacements of the structure solver. Therefore, one cannot speak of a real fluid-structure interaction. But the test showed successfully, that the developed program can be coupled via preCICE with a totally independent and different solver and is stable throughout the simulation. For the second coupling test, a real-world fluid solver developed in OpenFOAM was used for a fluid-structure interaction. %TODO ... weiter und letzter test fehlt
 
 % resumé: löser geeignet für multi-physics, wenn unterteilung des meshs entsprechend hoch ist, damit accuracy stimmt. dank gut skalierender parallelisierung möglich
  As a resum\'{e} the developed structure solver can be seen as qualified for multi-physics simulation under the assumption of a fine level of mesh subdivision for good accuracy. The good performance in parallel scaling makes it possible to provide such a fine subdivision level without risking to spend too much time for processing of the increased number of elements.

 \subsection{Future Development}
  The developed FEM-code successfully implemented flat shell elements and is able to work in coupled multi-physics simulations. The validation showed good accuracy for fine mesh subdivisions. The program was designed to be able to easily introduce new models for plane and plate elements, for example quadratic or cubic quadrilateral elements with 8 or 9 nodes. This would enhance the accuracy for coarser meshes further. The requirements of the developed program included two dimensional elements. One could expand this limit also to beam and truss elements of one dimension or to three dimensional solid elements represented by tetrahedral or hexahedral geometric elements. The libMesh library and the coupling tool preCICE would support such features.
  
  All scenarios considered in this work so far had constant conditions. The finite element idealization could be extended to simulations that are time dependent and simultaneously add dynamic behavior to the elastic structures. When displacements of an elastic body vary with time two additional forces must be introduced, namely the inertia or acceleration and resistance opposing the motion. Therefore, a new type of system solver must be introduced as well.
\newpage